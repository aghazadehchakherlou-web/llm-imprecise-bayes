# HIP-LLM: A Hierarchical Imprecise Probability Approach to Reliability Assessment of Large Language Models

A tiny, reproducible bundle of **numerics** used in all figures of the paper (imprecise hierarchical Bayes over LLM accuracies).

üìÑ **Paper**: [arXiv](link-when-available) | [PDF](link-when-available)

---

## üìë Table of Contents
- [Quick Start](#quick-start)
- [Authors](#-authors)
- [Overview](#-overview)
- [Abstract](#-abstract)
- [Visualizations](#-visualizations)

---

## Quick Start

Clone the repo and install the tiny runtime dependencies:

```bash
git clone https://github.com/aghazadehchakherlou-web/llm-imprecise-bayes.git
cd llm-imprecise-bayes
pip install -r requirements.txt
```

---

## üë• Authors

- [Robab Aghazadeh Chakherlou](https://scholar.google.co.uk/citations?hl=en&user=hfn5eFMAAAAJ)
- [Qing Guo](https://scholar.google.co.uk/citations?hl=en&user=joK9NAoAAAAJ)
- [Siddartha Khastgir](https://scholar.google.co.uk/citations?hl=en&user=r3ldU6sAAAAJ)
- [Peter Popov](https://scholar.google.co.uk/citations?hl=en&user=ZraM0uwAAAAJ)
- [Xiaoge Zhang](https://scholar.google.co.uk/citations?hl=en&user=USCW004AAAAJ)
- [Xingyu Zhao](https://scholar.google.co.uk/citations?hl=en&user=SzEBdA8AAAAJ)

---

## üåê Overview

### General Structure:
This figure provides a high-level overview of HIP-LLM, with an emphasis on how the operational profile shapes the analysis. It shows how an LLM is broken into multiple domains, each made up of several subdomains. Subdomains within the same domain share information, while different domains remain independent. This layered structure supports reliability inference from subdomains up to the full LLM ( subdomain ‚Üí domain ‚Üí LLM).

<p align="center">
  <img src="./General_Structure_2.PNG" width="80%">
</p>

### Hierarchical Bayesian Model: This visualization corresponds to the detailed model. It illustrates the full bottom-up Bayesian inference flow.
This diagram illustrates the full hierarchical model used in HIP-LLM. It shows how subdomain results feed into domain-level parameters, which are then combined using operational profiles to estimate domain and system-level reliability. It highlights how uncertainty is represented and propagated through the hierarchy.
<p align="center">
 <img src="./hierarchical_Bayes_imprecise_dependentSub_independentDomain-hyperInterval_2.PNG" width="80%">
</p>

---

## üìù Abstract

Large Language Models (LLMs) are increasingly deployed across diverse domains, raising the need for rigorous reliability assessment methods. Existing benchmark-based evaluations primarily offer descriptive statistics of model accuracy over datasets, providing limited insight into the probabilistic behavior of LLMs under real operational conditions. This paper introduces HIP-LLM, a Hierarchical Imprecise Probability framework for modeling and inferring LLM reliability. Building upon the foundations of software reliability engineering, HIP-LLM defines LLM reliability as the probability of failure-free operation over a specified number of future tasks under a given Operational Profile (OP). HIP-LLM represents dependencies across (sub-)domains hierarchically, enabling multi-level inference from subdomain to system-level reliability. HIP-LLM embeds imprecise priors to capture epistemic uncertainty and incorporates OPs to reflect usage contexts. It derives posterior reliability envelopes that quantify uncertainty across priors and data. Experiments on multiple benchmark datasets demonstrate that HIP-LLM offers a more accurate and standardized reliability characterization than existing benchmark and state-of-the-art approaches. A publicly accessible repository of HIP-LLM is provided.

---

## üìä Visualizations

### Subdomain Structure:
This figure focuses on the lowest layer of the hierarchy. Each subdomain contributes its own performance data, which forms the basis for estimating subdomain-level accuracy. These estimates serve as the starting point for all higher-level reliability analysis.
<p align="center">
  <img src="./subdomain.PNG" width="80%">
</p>

### Domain Structure:
This figure shows how subdomain results within the same domain are combined. The model treats subdomains in a domain as related, so evidence from one subdomain influences others. The domain structure summarizes how these dependent subdomains contribute to an overall domain-level reliability estimate.
<p align="center">
  <img src="./domain.PNG" width="80%">
</p>

### LLM-Level Reliability:
This visualization shows how domain-level results come together to produce the overall reliability of the LLM. Each domain contributes according to its operational importance. The figure highlights how uncertainty at lower levels accumulates into the system-level reliability envelope.
<p align="center">
  <img src="./LLM.PNG" width="40%">
</p>

### Operational Weights:
This figure shows the operational profiles used in the analysis. These weights reflect how frequently different subdomains and domains are expected to occur in real-world usage. They ensure that the final reliability estimate represents realistic operational conditions.
<p align="center">
  <img src="./weight.PNG" width="80%">
</p>

### Hyperparameters:
This figure presents the ranges of prior assumptions used in the imprecise probability model. The hyperparameters capture uncertainty about domain behaviour before observing data, and varying them produces a family of posterior results that reflect epistemic uncertainty.
<p align="center">
  <img src="./hyperparameter.PNG" width="80%">
</p>

### Reliability:
This figure summarizes the final reliability results of HIP-LLM. It shows how the LLM‚Äôs expected failure-free performance changes over future tasks, along with the uncertainty range arising from both data and prior assumptions. It provides a clear view of the LLM's reliability under realistic usage.
<p align="center">
  <img src="./reliability.PNG" width="80%">
</p>

### Comparison to baselines:
Tables 4 and 5 summarize how system-level reliability estimates vary with prior assumptions, operational profile (OP) specification, and sample size. In the Small-N regime (Table 4), all methods slightly underestimate the ground-truth reliability, and both OP mismatch and prior choice materially affect results: informative priors can reduce point error when correctly aligned, while dataset-based or approximated OPs increase bias. Unlike point-estimate baselines, HIP-LLM reports envelopes of plausible medians and credible intervals, explicitly capturing epistemic uncertainty due to prior and OP ambiguity. In the Large-N regime (Table 5), posterior estimates converge across all methods and OP scenarios, with narrow intervals indicating data dominance; correspondingly, HIP-LLM‚Äôs envelopes collapse to tight bounds, showing that prior uncertainty becomes negligible when sufficient evidence is available.
<p align="center">
  <img src="./table_4.PNG" width="80%">
</p>

<p align="center">
  <img src="./table_5.PNG" width="80%">
</p>

### Sensitivity to the definition of success/failure:
This experiment evaluates the sensitivity of HIP-LLM to how ‚Äúsuccess‚Äù is defined at the subdomain level by comparing Pass@1 and Pass@3 on a controlled setting (MBPP with Claude Sonnet 4.5). While the more permissive Pass@3 criterion increases observed success counts and shifts the posterior CDF envelope to higher reliability values, Figure 9 shows substantial overlap between the Pass@1 and Pass@3 posterior envelopes. This overlap indicates that HIP-LLM‚Äôs inferred reliability is only moderately sensitive to the specific success definition, demonstrating robustness to reasonable variations in evaluation criteria.
<p align="center">
  <img src="./Pass1_vs_Pass3.png" width="80%">
</p>

### Robustness to Memory Effects:
Figure 10 examines the impact of memory-induced dependence on HIP-LLM‚Äôs reliability estimates. Figure 10a shows that, when conversational context is retained across tasks, memory grows monotonically with the number of queries, providing a practical proxy for increasing dependence. Figures 10b‚Äì10d then demonstrate how injecting this dependence into the BoolQ subdomain produces smooth, rightward shifts in posterior CDF envelopes at the subdomain, domain, and LLM levels as the assumed accuracy increases. Importantly, while dependence measurably propagates through the hierarchy, its effect remains controlled and does not qualitatively alter the overall reliability characterization, indicating that HIP-LLM‚Äôs posterior conclusions are robust to realistic departures from the i.i.d. assumption.
<p align="center">
  <img src="./fig_10.PNG" width="80%">
</p>

### Scalability:
Figures 11a‚Äì11e empirically confirm the theoretical time-complexity analysis of HIP-LLM, showing that runtime scales as predicted with the number of domains, subdomains, hyperparameter configurations, Monte Carlo samples, and grid size, with subdomain-level inference clearly dominating the cost. The baseline timing breakdown in Fig. 11f further shows that subdomain posterior computation accounts for over 99% of total runtime, while domain- and LLM-level aggregation are negligible, explaining the weak dependence on the number of Monte Carlo samples. Consistently, Table 6 shows that peak memory usage scales approximately linearly with the number of domains, subdomains, hyperparameter configurations, and samples, but is effectively independent of the grid size, confirming efficient reuse of the hyperparameter grid.

<p align="center">
  <img src="./Fig11_RQ8.png" width="80%">
</p>

<p align="center">
  <img src="./Table 6_RQ8.png" width="80%">
</p>
---

## üì¶ Repository Structure

```
llm-imprecise-bayes/
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ General_Structure.PNG
‚îú‚îÄ‚îÄ hierarchical_Bayes_imprecise_dependentSub_i.PNG
‚îú‚îÄ‚îÄ subdomain.PNG
‚îú‚îÄ‚îÄ domain.PNG
‚îú‚îÄ‚îÄ LLM.PNG
‚îú‚îÄ‚îÄ weight.PNG
‚îú‚îÄ‚îÄ hyperparameter.PNG
‚îî‚îÄ‚îÄ reliability.PNG
```

---

## üìÑ Citation

If you use this work, please cite:

```bibtex
@article{aghazadeh2025hipllm,
  title={HIP-LLM: A Hierarchical Imprecise Probability Approach to Reliability Assessment of Large Language Models},
  author={Aghazadeh Chakherlou, Robab and Guo, Qing and Khastgir, Siddartha and Popov, Peter and Zhang, Xiaoge and Zhao, Xingyu},
  year={2025}
}
```

---

## üìß Contact

For questions or collaborations, please reach out to the authors via their Google Scholar profiles listed above.
